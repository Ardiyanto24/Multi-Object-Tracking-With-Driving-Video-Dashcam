{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602aa9bb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-28T03:30:43.949625Z",
     "iopub.status.busy": "2026-01-28T03:30:43.949233Z",
     "iopub.status.idle": "2026-01-28T06:24:12.444970Z",
     "shell.execute_reply": "2026-01-28T06:24:12.444213Z"
    },
    "papermill": {
     "duration": 10408.502695,
     "end_time": "2026-01-28T06:24:12.446939",
     "exception": false,
     "start_time": "2026-01-28T03:30:43.944244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "‚ö†Ô∏è LIMIT ACTIVE: Mengambil 720 video dengan Stratified Sampling (Day/Night).\n",
      "   Melakukan Stratified Sampling...\n",
      "   Proporsi Asli: Day 522 | Night 439\n",
      "   Target Sample: Day 391 | Night 329\n",
      "Total Videos to Process: 720\n",
      "Train Videos: 576, Val Videos: 144\n",
      "Starting extraction (Stride=3, Size=(960, 540), Quality=100%)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac09c4334b5432e87dd691acb25cec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Videos:   0%|          | 0/720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generation complete!\n",
      "Total Images Saved: 46524\n",
      "Train images: 37310\n",
      "Val images:   9214\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ================= KONFIGURASI =================\n",
    "# Path input\n",
    "INPUT_PREPRO_DIR = Path(\"/kaggle/input/multi-object-detection-tracking-prepro-v2/preprocessed_v2\") \n",
    "VIDEO_DIR = Path(\"/kaggle/input/driving-video-with-object-tracking/bdd100k_videos_train_00/bdd100k/videos/train\")\n",
    "\n",
    "# Output dataset untuk YOLO\n",
    "DATASET_DIR = Path(\"/kaggle/working/dataset\")\n",
    "IMG_DIR = DATASET_DIR / \"images\"\n",
    "LBL_DIR = DATASET_DIR / \"labels\"\n",
    "\n",
    "# Mapping Class ID\n",
    "CLASS_MAP = {\n",
    "    'car': 0, 'pedestrian': 1, 'truck': 2, 'bus': 3,\n",
    "    'bicycle': 4, 'other vehicle': 5, 'rider': 6,\n",
    "    'motorcycle': 7, 'other person': 8, 'train': 9, 'trailer': 10\n",
    "}\n",
    "\n",
    "# Konfigurasi Split & Limit\n",
    "VAL_SPLIT = 0.2 \n",
    "SEED = 42\n",
    "LIMIT_VIDEOS = 720  # Sesuai request\n",
    "\n",
    "# --- KONFIGURASI BARU (SESUAI PILIHAN ANDA) ---\n",
    "FRAME_STRIDE = 3            # Ambil 1 frame setiap 3 frame\n",
    "TARGET_SIZE = (960, 540)    # Resize ke 960x540\n",
    "JPEG_QUALITY = 100          # Kualitas gambar 100%\n",
    "\n",
    "# ================= FUNGSI BANTUAN =================\n",
    "def create_yolo_dirs():\n",
    "    for split in ['train', 'val']:\n",
    "        (IMG_DIR / split).mkdir(parents=True, exist_ok=True)\n",
    "        (LBL_DIR / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_yolo_bbox(row, orig_w, orig_h):\n",
    "    # PENTING: Koordinat YOLO harus dinormalisasi berdasarkan ukuran ASLI (sebelum resize)\n",
    "    # karena posisi relatif objek (0.0 - 1.0) tidak berubah meskipun gambar di-resize.\n",
    "    dw = 1.0 / orig_w\n",
    "    dh = 1.0 / orig_h\n",
    "    \n",
    "    x_center = ((row['x1'] + row['x2']) / 2.0) * dw\n",
    "    y_center = ((row['y1'] + row['y2']) / 2.0) * dh\n",
    "    w = (row['x2'] - row['x1']) * dw\n",
    "    h = (row['y2'] - row['y1']) * dh\n",
    "    \n",
    "    return x_center, y_center, w, h\n",
    "\n",
    "def stratified_sample(all_videos, df_flags, n_samples):\n",
    "    \"\"\"\n",
    "    Mengambil sampel video dengan mempertahankan rasio Siang/Malam.\n",
    "    \"\"\"\n",
    "    print(\"   Melakukan Stratified Sampling...\")\n",
    "    \n",
    "    video_conditions = df_flags[df_flags['video'].isin(all_videos)].groupby('video')['is_night'].mean()\n",
    "    \n",
    "    day_videos = video_conditions[video_conditions <= 0.5].index.tolist()\n",
    "    night_videos = video_conditions[video_conditions > 0.5].index.tolist()\n",
    "    \n",
    "    total_avail = len(day_videos) + len(night_videos)\n",
    "    if n_samples >= total_avail:\n",
    "        return all_videos \n",
    "    \n",
    "    n_day = int(n_samples * (len(day_videos) / total_avail))\n",
    "    n_night = n_samples - n_day\n",
    "    \n",
    "    print(f\"   Proporsi Asli: Day {len(day_videos)} | Night {len(night_videos)}\")\n",
    "    print(f\"   Target Sample: Day {n_day} | Night {n_night}\")\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    selected_day = np.random.choice(day_videos, n_day, replace=False).tolist()\n",
    "    selected_night = np.random.choice(night_videos, n_night, replace=False).tolist()\n",
    "    \n",
    "    selected_videos = selected_day + selected_night\n",
    "    np.random.shuffle(selected_videos) \n",
    "    \n",
    "    return selected_videos\n",
    "\n",
    "def process_dataset():\n",
    "    # Cleanup dataset sebelumnya agar bersih\n",
    "    if DATASET_DIR.exists():\n",
    "        shutil.rmtree(DATASET_DIR)\n",
    "        \n",
    "    print(\"Loading data...\")\n",
    "    # 1. Load Manifest, Labels & Flags\n",
    "    manifest_path = INPUT_PREPRO_DIR / \"P6\" / \"train_manifest.json\"\n",
    "    labels_path = INPUT_PREPRO_DIR / \"P3\" / \"labels_clean_dropShort.parquet\"\n",
    "    flags_path = INPUT_PREPRO_DIR / \"P5\" / \"day_night_flags.parquet\" \n",
    "    \n",
    "    with open(manifest_path, 'r') as f:\n",
    "        manifest = json.load(f)\n",
    "        \n",
    "    df_labels = pd.read_parquet(labels_path)\n",
    "    df_flags = pd.read_parquet(flags_path)\n",
    "    \n",
    "    # Filter label\n",
    "    manifest_keys = set(f\"{m['video']}_{m['frameIndex']}\" for m in manifest)\n",
    "    df_labels['key'] = df_labels['video'] + \"_\" + df_labels['frameIndex'].astype(str)\n",
    "    df_labels = df_labels[df_labels['key'].isin(manifest_keys)].copy()\n",
    "    \n",
    "    # 2. Prepare Videos dengan Stratified Sampling\n",
    "    unique_videos = list(set(m['video'] for m in manifest))\n",
    "    \n",
    "    if LIMIT_VIDEOS is not None and LIMIT_VIDEOS < len(unique_videos):\n",
    "        print(f\"‚ö†Ô∏è LIMIT ACTIVE: Mengambil {LIMIT_VIDEOS} video dengan Stratified Sampling (Day/Night).\")\n",
    "        selected_videos = stratified_sample(unique_videos, df_flags, LIMIT_VIDEOS)\n",
    "    else:\n",
    "        selected_videos = unique_videos\n",
    "        np.random.seed(SEED)\n",
    "        np.random.shuffle(selected_videos)\n",
    "\n",
    "    # Split Train/Val\n",
    "    num_val = int(len(selected_videos) * VAL_SPLIT)\n",
    "    if num_val == 0 and len(selected_videos) > 1: num_val = 1\n",
    "        \n",
    "    val_videos = set(selected_videos[:num_val])\n",
    "    train_videos = set(selected_videos[num_val:])\n",
    "    \n",
    "    print(f\"Total Videos to Process: {len(selected_videos)}\")\n",
    "    print(f\"Train Videos: {len(train_videos)}, Val Videos: {len(val_videos)}\")\n",
    "    \n",
    "    # Group manifest by video \n",
    "    video_groups = {}\n",
    "    valid_video_set = set(selected_videos)\n",
    "    \n",
    "    for m in manifest:\n",
    "        if m['video'] in valid_video_set:\n",
    "            if m['video'] not in video_groups:\n",
    "                video_groups[m['video']] = []\n",
    "            video_groups[m['video']].append(m['frameIndex'])\n",
    "        \n",
    "    create_yolo_dirs()\n",
    "    \n",
    "    # 3. Extraction Loop\n",
    "    print(f\"Starting extraction (Stride={FRAME_STRIDE}, Size={TARGET_SIZE}, Quality={JPEG_QUALITY}%)...\")\n",
    "    \n",
    "    total_imgs = 0\n",
    "    \n",
    "    for video_name, frames in tqdm(video_groups.items(), desc=\"Processing Videos\"):\n",
    "        split = 'val' if video_name in val_videos else 'train'\n",
    "        \n",
    "        video_path = VIDEO_DIR / f\"{video_name}.mov\"\n",
    "        if not video_path.exists():\n",
    "            video_path = VIDEO_DIR / video_name \n",
    "        if not video_path.exists():\n",
    "            continue\n",
    "            \n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened(): continue\n",
    "            \n",
    "        frames = sorted(frames)\n",
    "        \n",
    "        # Gunakan enumerate untuk logika Stride\n",
    "        for i, frame_idx in enumerate(frames):\n",
    "            \n",
    "            # --- LOGIKA 1: FRAME STRIDE ---\n",
    "            # Jika sisa bagi index dengan FRAME_STRIDE bukan 0, skip frame ini.\n",
    "            if i % FRAME_STRIDE != 0:\n",
    "                continue\n",
    "\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, img = cap.read()\n",
    "            if not ret: continue\n",
    "                \n",
    "            h_orig, w_orig = img.shape[:2]\n",
    "            \n",
    "            # --- LOGIKA 2: RESIZE ---\n",
    "            # Resize gambar ke target size (960, 540)\n",
    "            img_resized = cv2.resize(img, TARGET_SIZE, interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            # Save Image\n",
    "            file_id = f\"{video_name}_{frame_idx:06d}\"\n",
    "            img_out_path = IMG_DIR / split / f\"{file_id}.jpg\"\n",
    "            \n",
    "            # --- LOGIKA 3: JPEG QUALITY ---\n",
    "            # Simpan dengan kualitas yang ditentukan\n",
    "            cv2.imwrite(str(img_out_path), img_resized, [int(cv2.IMWRITE_JPEG_QUALITY), JPEG_QUALITY])\n",
    "            \n",
    "            total_imgs += 1\n",
    "            \n",
    "            # Save Labels\n",
    "            frame_labels = df_labels[\n",
    "                (df_labels['video'] == video_name) & \n",
    "                (df_labels['frameIndex'] == frame_idx)\n",
    "            ]\n",
    "            \n",
    "            txt_out_path = LBL_DIR / split / f\"{file_id}.txt\"\n",
    "            with open(txt_out_path, 'w') as f_txt:\n",
    "                for _, row in frame_labels.iterrows():\n",
    "                    cls_name = row['category']\n",
    "                    if cls_name not in CLASS_MAP: continue\n",
    "                    cls_id = CLASS_MAP[cls_name]\n",
    "                    \n",
    "                    # NOTE: Koordinat bbox dihitung berdasarkan ukuran ASLI (w_orig, h_orig).\n",
    "                    # Ini benar karena koordinat YOLO adalah relatif (0.0 - 1.0).\n",
    "                    xc, yc, w, h = get_yolo_bbox(row, w_orig, h_orig)\n",
    "                    \n",
    "                    xc = max(0, min(1, xc))\n",
    "                    yc = max(0, min(1, yc))\n",
    "                    w = max(0, min(1, w))\n",
    "                    h = max(0, min(1, h))\n",
    "                    \n",
    "                    f_txt.write(f\"{cls_id} {xc:.6f} {yc:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "    print(\"Dataset generation complete!\")\n",
    "    print(f\"Total Images Saved: {total_imgs}\")\n",
    "    print(f\"Train images: {len(list((IMG_DIR/'train').glob('*.jpg')))}\")\n",
    "    print(f\"Val images:   {len(list((IMG_DIR/'val').glob('*.jpg')))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1ca586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T06:24:12.454640Z",
     "iopub.status.busy": "2026-01-28T06:24:12.453662Z",
     "iopub.status.idle": "2026-01-28T06:24:12.483991Z",
     "shell.execute_reply": "2026-01-28T06:24:12.483072Z"
    },
    "papermill": {
     "duration": 0.035749,
     "end_time": "2026-01-28T06:24:12.485750",
     "exception": false,
     "start_time": "2026-01-28T06:24:12.450001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File konfigurasi berhasil dibuat: /kaggle/working/bdd_mot.yaml\n",
      "\n",
      "Isi file bdd_mot.yaml:\n",
      "------------------------------\n",
      "path: /kaggle/working/dataset\n",
      "train: images/train\n",
      "val: images/val\n",
      "names:\n",
      "  0: car\n",
      "  1: pedestrian\n",
      "  2: truck\n",
      "  3: bus\n",
      "  4: bicycle\n",
      "  5: other vehicle\n",
      "  6: rider\n",
      "  7: motorcycle\n",
      "  8: other person\n",
      "  9: train\n",
      "  10: trailer\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# ================= KONFIGURASI =================\n",
    "# Pastikan ini SAMA PERSIS dengan Step 1\n",
    "CLASS_MAP = {\n",
    "    'car': 0, 'pedestrian': 1, 'truck': 2, 'bus': 3,\n",
    "    'bicycle': 4, 'other vehicle': 5, 'rider': 6,\n",
    "    'motorcycle': 7, 'other person': 8, 'train': 9, 'trailer': 10\n",
    "}\n",
    "\n",
    "# Konversi ke format yang diminta YOLO: {0: 'car', 1: 'pedestrian', ...}\n",
    "# Kita perlu membalik key dan value dari CLASS_MAP\n",
    "NAMES_MAP = {v: k for k, v in CLASS_MAP.items()}\n",
    "\n",
    "# Path output file yaml\n",
    "YAML_OUTPUT_PATH = '/kaggle/working/bdd_mot.yaml'\n",
    "\n",
    "# ================= PEMBUATAN YAML =================\n",
    "# Struktur Dictionary untuk YAML\n",
    "data_yaml = {\n",
    "    'path': '/kaggle/working/dataset',  # Base path dataset (Absolute path)\n",
    "    'train': 'images/train',            # Folder training (relative to path)\n",
    "    'val': 'images/val',                # Folder validasi (relative to path)\n",
    "    'names': NAMES_MAP                  # Mapping ID ke Nama Kelas\n",
    "}\n",
    "\n",
    "# Tulis ke file\n",
    "with open(YAML_OUTPUT_PATH, 'w') as f:\n",
    "    yaml.dump(data_yaml, f, sort_keys=False)\n",
    "\n",
    "print(f\"‚úÖ File konfigurasi berhasil dibuat: {YAML_OUTPUT_PATH}\")\n",
    "print(\"\\nIsi file bdd_mot.yaml:\")\n",
    "print(\"-\" * 30)\n",
    "with open(YAML_OUTPUT_PATH, 'r') as f:\n",
    "    print(f.read())\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c557956a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T06:24:12.491976Z",
     "iopub.status.busy": "2026-01-28T06:24:12.491708Z",
     "iopub.status.idle": "2026-01-28T06:24:18.829263Z",
     "shell.execute_reply": "2026-01-28T06:24:18.828365Z"
    },
    "papermill": {
     "duration": 6.342903,
     "end_time": "2026-01-28T06:24:18.831248",
     "exception": false,
     "start_time": "2026-01-28T06:24:12.488345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\r\n",
      "  Downloading ultralytics-8.4.8-py3-none-any.whl.metadata (38 kB)\r\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.5)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.15.3)\r\n",
      "Requirement already satisfied: torch<2.10,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\r\n",
      "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\r\n",
      "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\r\n",
      "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (26.0rc2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.6.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.20.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.4.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.10,>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.10,>=1.8.0->ultralytics) (3.0.3)\r\n",
      "Downloading ultralytics-8.4.8-py3-none-any.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\r\n",
      "Installing collected packages: ultralytics-thop, ultralytics\r\n",
      "Successfully installed ultralytics-8.4.8 ultralytics-thop-2.0.18\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6909a228",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T06:24:18.839826Z",
     "iopub.status.busy": "2026-01-28T06:24:18.839290Z",
     "iopub.status.idle": "2026-01-28T08:21:31.120771Z",
     "shell.execute_reply": "2026-01-28T08:21:31.119935Z"
    },
    "papermill": {
     "duration": 7032.287873,
     "end_time": "2026-01-28T08:21:31.122370",
     "exception": false,
     "start_time": "2026-01-28T06:24:18.834497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "üî• GPU Available: True\n",
      "   Device: Tesla P100-PCIE-16GB\n",
      "Loading model...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8s.pt to 'yolov8s.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.5MB 224.8MB/s 0.1s\n",
      "Starting training (30 Epochs with Imbalance Strategy)...\n",
      "WARNING ‚ö†Ô∏è 'label_smoothing' is deprecated and will be removed in the future.\n",
      "Ultralytics 8.4.8 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/working/bdd_mot.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=30, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=yolov8s_run_30e, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=bdd_mot_project, rect=True, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/runs/detect/bdd_mot_project/yolov8s_run_30e, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 27.9MB/s 0.0s\n",
      "Overriding model.yaml nc=80 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2120305  ultralytics.nn.modules.head.Detect           [11, 16, None, [128, 256, 512]]\n",
      "Model summary: 130 layers, 11,139,857 parameters, 11,139,841 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.3MB 88.9MB/s 0.1s\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2105.8¬±1417.8 MB/s, size: 286.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/dataset/labels/train... 37310 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 37310/37310 998.9it/s 37.4s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1535.4¬±1080.2 MB/s, size: 285.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/dataset/labels/val... 9214 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9214/9214 957.7it/s 9.6s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/dataset/labels/val.cache\n",
      "Plotting labels to /kaggle/working/runs/detect/bdd_mot_project/yolov8s_run_30e/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/kaggle/working/runs/detect/bdd_mot_project/yolov8s_run_30e\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/30      2.36G      2.917       3.06      1.918        128        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 3.3it/s 11:52\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.4it/s 1:05\n",
      "                   all       9214      84489      0.114      0.022    0.00837    0.00365\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/30      2.67G      2.824      2.799      1.845        119        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 3.9it/s 9:58\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.7it/s 1:01\n",
      "                   all       9214      84489      0.193     0.0225    0.00559    0.00212\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/30      2.71G      2.861       2.85      1.879        121        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.2it/s 9:11\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.6it/s 1:02\n",
      "                   all       9214      84489     0.0124    0.00859    0.00452    0.00159\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/30      2.74G      2.845      2.841      1.884        150        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.3it/s 9:06\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.6it/s 1:02\n",
      "                   all       9214      84489      0.101     0.0121    0.00534    0.00206\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/30      2.78G      2.765       2.75      1.831        133        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.3it/s 9:06\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.7it/s 1:02\n",
      "                   all       9214      84489      0.294    0.00958    0.00568    0.00214\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/30      2.82G      2.704      2.674       1.79        110        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.3it/s 9:02\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.8it/s 1:01\n",
      "                   all       9214      84489      0.198    0.00881    0.00528    0.00209\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/30      2.85G      2.657      2.614      1.764        126        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.3it/s 8:60\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.8it/s 1:00\n",
      "                   all       9214      84489      0.194     0.0104    0.00501    0.00182\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/30      2.89G      2.614      2.547      1.734        158        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.3it/s 8:59\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.7it/s 1:02\n",
      "                   all       9214      84489      0.378    0.00957    0.00549    0.00199\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/30      2.92G      2.573      2.493       1.71        125        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.3it/s 9:05\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 4.7it/s 1:02\n",
      "                   all       9214      84489      0.376    0.00991    0.00499    0.00177\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/30      2.96G      2.536      2.436      1.685        114        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.3it/s 9:02\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 5.2it/s 55.5s\n",
      "                   all       9214      84489      0.565    0.00981    0.00476    0.00181\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/30         3G      2.506      2.395      1.662        130        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2332/2332 4.5it/s 8:35\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 5.3it/s 54.2s\n",
      "                   all       9214      84489      0.483    0.00904    0.00491    0.00181\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 1, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "11 epochs completed in 1.914 hours.\n",
      "Optimizer stripped from /kaggle/working/runs/detect/bdd_mot_project/yolov8s_run_30e/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from /kaggle/working/runs/detect/bdd_mot_project/yolov8s_run_30e/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating /kaggle/working/runs/detect/bdd_mot_project/yolov8s_run_30e/weights/best.pt...\n",
      "Ultralytics 8.4.8 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Model summary (fused): 73 layers, 11,129,841 parameters, 0 gradients, 28.5 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 288/288 5.0it/s 57.8s\n",
      "                   all       9214      84489      0.115     0.0211    0.00835    0.00365\n",
      "                   car       9129      64279      0.055      0.193     0.0458     0.0164\n",
      "            pedestrian       3354      11539     0.0209    0.00607    0.00373   0.000952\n",
      "                 truck       2629       4092     0.0373    0.00733    0.00601    0.00212\n",
      "                   bus       1587       2193       0.14      0.021     0.0347       0.02\n",
      "               bicycle        704        890    0.00946    0.00562    0.00158   0.000666\n",
      "         other vehicle        398        440          0          0          0          0\n",
      "                 rider        494        588          0          0          0          0\n",
      "            motorcycle        293        355          0          0          0          0\n",
      "          other person         11         11          0          0          0          0\n",
      "                 train         26         26          1          0          0          0\n",
      "               trailer         76         76          0          0          0          0\n",
      "Speed: 0.1ms preprocess, 2.6ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1m/kaggle/working/runs/detect/bdd_mot_project/yolov8s_run_30e\u001b[0m\n",
      "‚úÖ Training Selesai!\n",
      "   Best Model Saved at: /kaggle/working/runs/detect/bdd_mot_project/yolov8s_run_30e/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# ================= KONFIGURASI =================\n",
    "# Matikan WandB agar tidak perlu login (offline mode)\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "# Bersihkan Memori GPU sebelum mulai\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Cek GPU\n",
    "print(f\"üî• GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ================= TRAINING LOOP =================\n",
    "def train_yolo():\n",
    "    # 1. Load Model\n",
    "    # Gunakan 'yolov8s.pt' (small) untuk keseimbangan performa/speed yang baik\n",
    "    print(\"Loading model...\")\n",
    "    model = YOLO('yolov8s.pt') \n",
    "\n",
    "    # 2. Start Training\n",
    "    print(\"Starting training (30 Epochs with Imbalance Strategy)...\")\n",
    "    results = model.train(\n",
    "        data='/kaggle/working/bdd_mot.yaml', # Path ke file yaml dari Step 2\n",
    "        \n",
    "        # --- DURASI TRAINING ---\n",
    "        epochs=30,        # REQUEST: Naik ke 30 epoch\n",
    "        patience=10,      # Stop jika tidak ada perbaikan dalam 10 epoch (Early Stopping)\n",
    "        \n",
    "        # --- INPUT CONFIG ---\n",
    "        imgsz=640,        # Resolusi input (Standard YOLOv8)\n",
    "        batch=16,         # Batch size (Jika OOM, turunkan ke 8)\n",
    "        \n",
    "        # --- STRATEGI UNTUK KELAS MINORITAS & IMBALANCE ---\n",
    "        rect=True,           # Wajib untuk dataset mengemudi (aspek rasio lebar)\n",
    "        cos_lr=True,         # Cosine LR scheduler (Membantu konvergensi di epoch panjang)\n",
    "        label_smoothing=0.1, # Regularisasi: Mencegah overfitting pada kelas mayoritas (Car)\n",
    "        close_mosaic=10,     # Matikan augmentasi Mosaic di 10 epoch terakhir (Fokus objek utuh)\n",
    "        \n",
    "        # --- SYSTEM CONFIG ---\n",
    "        workers=4,        \n",
    "        optimizer='auto', \n",
    "        \n",
    "        # --- LOGGING ---\n",
    "        project='bdd_mot_project', \n",
    "        name='yolov8s_run_30e',   # Ganti nama agar tahu ini run 30 epoch\n",
    "        exist_ok=True,    \n",
    "        plots=True,       \n",
    "        save=True,        \n",
    "        val=True          \n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Training Selesai!\")\n",
    "    # Kita gunakan properti save_dir agar dinamis (tidak hardcode path)\n",
    "    print(f\"   Best Model Saved at: {results.save_dir}/weights/best.pt\")\n",
    "    return model, results\n",
    "\n",
    "# Eksekusi\n",
    "if __name__ == \"__main__\":\n",
    "    model, results = train_yolo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee25f92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T08:21:33.493600Z",
     "iopub.status.busy": "2026-01-28T08:21:33.492569Z",
     "iopub.status.idle": "2026-01-28T08:21:37.411536Z",
     "shell.execute_reply": "2026-01-28T08:21:37.410551Z"
    },
    "papermill": {
     "duration": 5.048957,
     "end_time": "2026-01-28T08:21:37.413262",
     "exception": false,
     "start_time": "2026-01-28T08:21:32.364305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install library supervision untuk visualisasi tracking yang keren\n",
    "!pip install supervision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb882e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T08:21:39.919805Z",
     "iopub.status.busy": "2026-01-28T08:21:39.918964Z",
     "iopub.status.idle": "2026-01-28T08:21:40.325828Z",
     "shell.execute_reply": "2026-01-28T08:21:40.324976Z"
    },
    "papermill": {
     "duration": 1.666571,
     "end_time": "2026-01-28T08:21:40.327372",
     "exception": false,
     "start_time": "2026-01-28T08:21:38.660801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Processing Video: 015fe6c9-48a58255.mov\n",
      "üß† Loading Model: /kaggle/working/bdd_mot_project/yolov8s_run_test/weights/best.pt\n",
      "‚ùå Error: Model path tidak ditemukan. Pastikan training Step 3 sukses.\n"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ================= KONFIGURASI =================\n",
    "# Path Model (Pastikan path ini sesuai dengan output Step 3)\n",
    "MODEL_PATH = '/kaggle/working/bdd_mot_project/yolov8s_run_test/weights/best.pt'\n",
    "\n",
    "# Path Video Test\n",
    "# Kita ambil salah satu video dari folder train asli (karena kita tidak mindahin video ke folder val)\n",
    "VIDEO_DIR = \"/kaggle/input/driving-video-with-object-tracking/bdd100k_videos_train_00/bdd100k/videos/train\"\n",
    "\n",
    "# Pilih video acak untuk tes (atau ganti nama file spesifik jika mau)\n",
    "# Kita cari video yang ada di list validasi (opsional, acak saja untuk demo)\n",
    "all_videos = [f for f in os.listdir(VIDEO_DIR) if f.endswith('.mov')]\n",
    "TEST_VIDEO_NAME = random.choice(all_videos) \n",
    "SOURCE_VIDEO_PATH = os.path.join(VIDEO_DIR, TEST_VIDEO_NAME)\n",
    "\n",
    "# Output Path\n",
    "OUTPUT_VIDEO_PATH = f\"/kaggle/working/output_tracking_{TEST_VIDEO_NAME.replace('.mov', '.mp4')}\"\n",
    "\n",
    "print(f\"üé¨ Processing Video: {TEST_VIDEO_NAME}\")\n",
    "print(f\"üß† Loading Model: {MODEL_PATH}\")\n",
    "\n",
    "# ================= TRACKING PIPELINE =================\n",
    "def run_tracking():\n",
    "    # 1. Load Model\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"‚ùå Error: Model path tidak ditemukan. Pastikan training Step 3 sukses.\")\n",
    "        return\n",
    "\n",
    "    model = YOLO(MODEL_PATH)\n",
    "\n",
    "    # 2. Setup Video Info\n",
    "    video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
    "    \n",
    "    # 3. Setup Annotators (Supervision)\n",
    "    # TraceAnnotator: Menggambar jejak garis gerakan\n",
    "    trace_annotator = sv.TraceAnnotator(\n",
    "        trace_length=60, \n",
    "        thickness=2\n",
    "    )\n",
    "    # BoxAnnotator: Menggambar kotak\n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "        thickness=2\n",
    "    )\n",
    "    # LabelAnnotator: Menulis Class Name + Track ID\n",
    "    label_annotator = sv.LabelAnnotator(\n",
    "        text_scale=0.5,\n",
    "        text_thickness=1,\n",
    "        text_padding=5\n",
    "    )\n",
    "\n",
    "    # 4. Open Video Sink (Writer)\n",
    "    # Kita limit 300 frame (10 detik) saja agar proses cepat untuk demo\n",
    "    MAX_FRAMES = 300 \n",
    "    \n",
    "    with sv.VideoSink(target_path=OUTPUT_VIDEO_PATH, video_info=video_info) as sink:\n",
    "        \n",
    "        # Loop Inference menggunakan Generator model.track\n",
    "        results_generator = model.track(\n",
    "            source=SOURCE_VIDEO_PATH, \n",
    "            persist=True,        # PENTING: Mengaktifkan memori antar-frame untuk tracking\n",
    "            tracker=\"bytetrack.yaml\", # Default tracker config\n",
    "            conf=0.3,            # Confidence threshold\n",
    "            iou=0.5,             # IoU threshold\n",
    "            stream=True,         # Generator mode (hemat memori)\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(\"üöÄ Starting Inference & Rendering...\")\n",
    "        \n",
    "        for i, result in enumerate(results_generator):\n",
    "            if i >= MAX_FRAMES: \n",
    "                break\n",
    "                \n",
    "            # Konversi hasil YOLO ke format Supervision\n",
    "            detections = sv.Detections.from_ultralytics(result)\n",
    "            \n",
    "            # Ambil Track ID (jika ada)\n",
    "            if result.boxes.id is not None:\n",
    "                detections.tracker_id = result.boxes.id.cpu().numpy().astype(int)\n",
    "                \n",
    "            # Filter class (opsional, misal hanya mobil & orang)\n",
    "            # detections = detections[detections.class_id != 0] \n",
    "\n",
    "            # Construct Labels: \"ID: Class Conf\"\n",
    "            labels = []\n",
    "            for tracker_id, class_id, confidence in zip(detections.tracker_id, detections.class_id, detections.confidence):\n",
    "                class_name = model.model.names[class_id]\n",
    "                labels.append(f\"#{tracker_id} {class_name} {confidence:.2f}\")\n",
    "\n",
    "            # Annotate Frame\n",
    "            frame = result.orig_img.copy()\n",
    "            \n",
    "            # Gambar Jejak dulu (di bawah kotak)\n",
    "            annotated_frame = trace_annotator.annotate(\n",
    "                scene=frame, detections=detections\n",
    "            )\n",
    "            # Gambar Kotak\n",
    "            annotated_frame = box_annotator.annotate(\n",
    "                scene=annotated_frame, detections=detections\n",
    "            )\n",
    "            # Gambar Label\n",
    "            annotated_frame = label_annotator.annotate(\n",
    "                scene=annotated_frame, detections=detections, labels=labels\n",
    "            )\n",
    "\n",
    "            # Simpan Frame ke Video\n",
    "            sink.write_frame(annotated_frame)\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(f\"   Processed frame {i}/{MAX_FRAMES}...\")\n",
    "\n",
    "    print(f\"‚úÖ Selesai! Video tersimpan di: {OUTPUT_VIDEO_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ab5788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-28T08:21:42.701521Z",
     "iopub.status.busy": "2026-01-28T08:21:42.700274Z",
     "iopub.status.idle": "2026-01-28T08:21:42.706265Z",
     "shell.execute_reply": "2026-01-28T08:21:42.705650Z"
    },
    "papermill": {
     "duration": 1.242241,
     "end_time": "2026-01-28T08:21:42.707590",
     "exception": false,
     "start_time": "2026-01-28T08:21:41.465349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video not found.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def show_video(video_path):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(\"Video not found.\")\n",
    "        return\n",
    "        \n",
    "    mp4 = open(video_path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(f\"\"\"\n",
    "    <video width=640 controls>\n",
    "          <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n",
    "\n",
    "# Tampilkan video\n",
    "show_video(OUTPUT_VIDEO_PATH)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2231353,
     "sourceId": 3731962,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 293787505,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17465.925225,
   "end_time": "2026-01-28T08:21:46.869097",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-28T03:30:40.943872",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1719e0bc8ff5418d8a8f547cfe34fd39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2836f2e90eb7472a80f6ca39f16e388f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8ca908f0c7e14ce7a9b75e894a9b951e",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_7a5d25bc1e794fdd8abdaf1eb375043a",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá720/720‚Äá[2:53:20&lt;00:00,‚Äá15.01s/it]"
      }
     },
     "433be73ef73043c7b8f819e8c59d9885": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a5d25bc1e794fdd8abdaf1eb375043a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "801f6fe3090c4d508042765811b626dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f8ca8f8acb4740c59ee5f76b25fb34ce",
       "max": 720.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e6742e2a64c148e7bef79a65449c91f7",
       "tabbable": null,
       "tooltip": null,
       "value": 720.0
      }
     },
     "8ca908f0c7e14ce7a9b75e894a9b951e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bac09c4334b5432e87dd691acb25cec7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f84bd2f4b62742ad8f57a968961d66dc",
        "IPY_MODEL_801f6fe3090c4d508042765811b626dc",
        "IPY_MODEL_2836f2e90eb7472a80f6ca39f16e388f"
       ],
       "layout": "IPY_MODEL_433be73ef73043c7b8f819e8c59d9885",
       "tabbable": null,
       "tooltip": null
      }
     },
     "bb1b4741aa064629a1512977200b7d40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6742e2a64c148e7bef79a65449c91f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f84bd2f4b62742ad8f57a968961d66dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bb1b4741aa064629a1512977200b7d40",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_1719e0bc8ff5418d8a8f547cfe34fd39",
       "tabbable": null,
       "tooltip": null,
       "value": "Processing‚ÄáVideos:‚Äá100%"
      }
     },
     "f8ca8f8acb4740c59ee5f76b25fb34ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
